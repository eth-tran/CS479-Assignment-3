{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A3-Q3: Combatting Overfitting with Dropout and Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": []
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pylab as plt\n",
        "import copy\n",
        "\n",
        "#for reproducibility purposes\n",
        "torch.manual_seed(2025)\n",
        "np.random.seed(2025)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DividedPlane(torch.utils.data.Dataset):\n",
        "    def __init__(self, n=100, noise=0.1, seed=None):\n",
        "        torch.manual_seed(seed)\n",
        "        theta = torch.rand((1,))*2.*torch.pi\n",
        "        a = torch.tensor([torch.cos(theta), torch.sin(theta), 0.1])\n",
        "        def myfunc(x):\n",
        "            y = a[0]*x[:,0] + a[1]*x[:,1] + a[2]\n",
        "            return y\n",
        "        self.x = torch.rand((n,2))*2. - 1.\n",
        "        y = myfunc(self.x) + noise*torch.normal( torch.zeros((len(self.x))) )\n",
        "        self.y = (y>0.).type(torch.float)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "    \n",
        "    def inputs(self):\n",
        "        return self.x\n",
        "    \n",
        "    def targets(self):\n",
        "        return self.y.reshape( (len(self.y),1) )\n",
        "    \n",
        "    def plot(self, labels=None, *args, **kwargs): \n",
        "        X = self.inputs()\n",
        "        if labels is None:\n",
        "            labels = self.targets()\n",
        "        colour_options = ['y', 'r', 'g', 'b', 'k']\n",
        "        if len(labels[0])>1:\n",
        "            # one-hot labels\n",
        "            cidx = torch.argmax(labels, axis=1)\n",
        "        else:\n",
        "            # binary labels\n",
        "            cidx = (labels>0.5).type(torch.int)\n",
        "        colours = [colour_options[k] for k in cidx]\n",
        "        plt.scatter(X[:,0].detach(), X[:,1].detach(), color=colours, marker='.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = np.random.randint(100000)\n",
        "train = DividedPlane(n=50, noise=0.25, seed=seed)\n",
        "validation = DividedPlane(n=5000, noise=0.25, seed=seed)\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.subplot(1,2,1); train.plot(); plt.title(f'Training Set');\n",
        "plt.subplot(1,2,2); validation.plot(); plt.title(f'Validation Set');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (a): `Dropout` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dropout(torch.nn.Module):\n",
        "    '''\n",
        "     lyr = Dropout()\n",
        "     \n",
        "     Creates a dropout layer in which each node is set to zero\n",
        "     with probability lyr.dropprob.\n",
        "     \n",
        "     Usage:\n",
        "       lyr = Dropout()\n",
        "       lyr.set_dropprob(p) # set the dropout probability to p\n",
        "       y = lyr(z)          # sets each node to 0 with probability p\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dropprob = 0.\n",
        "        \n",
        "    def set_dropprob(self, p):\n",
        "        self.dropprob = p\n",
        "        \n",
        "    def forward(self, z):\n",
        "        # Drop nodes with prob dropprob\n",
        "        #===== YOUR CODE HERE =====\n",
        "        y = z  # replace this line\n",
        "        \n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test for Dropout layer\n",
        "z = torch.ones((3,1000))\n",
        "drop_layer = Dropout()\n",
        "drop_layer.set_dropprob(0.75)\n",
        "y = drop_layer(z)\n",
        "drop_fraction = (torch.sum(y==0.)*100.)/torch.numel(y)\n",
        "print(f'Dropped {drop_fraction:.1f}%')\n",
        "print(f'Expected output is {torch.sum(y)}, which should be close to {torch.sum(z)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (b): `RobustNetwork`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RobustNetwork(torch.nn.Module):\n",
        "    def __init__(self, nodes=100):\n",
        "        super().__init__()\n",
        "        self.lyrs = torch.nn.ModuleList()\n",
        "        self.lyrs.append(torch.nn.Linear(2, nodes//2))\n",
        "        self.lyrs.append(torch.nn.Tanh())\n",
        "        self.drop_lyr1 = Dropout()\n",
        "        self.lyrs.append(self.drop_lyr1)\n",
        "        self.lyrs.append(torch.nn.Linear(nodes//2, nodes))\n",
        "        self.lyrs.append(torch.nn.Tanh())\n",
        "        self.drop_lyr2 = Dropout()   \n",
        "        self.lyrs.append(self.drop_lyr2)  \n",
        "        self.lyrs.append(torch.nn.Linear(nodes, 1))\n",
        "        self.lyrs.append(torch.nn.Sigmoid())\n",
        "        self.loss_fcn = torch.nn.BCELoss(reduction='mean')\n",
        "    \n",
        "    def forward(self, x):\n",
        "        y = x\n",
        "        for lyr in self.lyrs:\n",
        "            y = lyr(y)\n",
        "        return y\n",
        "    \n",
        "    def learn(self, x, t, epochs=100, lr=0.1, l2_lambda=0.0):\n",
        "        losses = []\n",
        "        for epoch in range(epochs):\n",
        "            y = self(x)\n",
        "            loss = self.loss_fcn(y.squeeze(), t.squeeze())\n",
        "\n",
        "            #Add l2 regulaization implementation here\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            self.zero_grad()\n",
        "            loss.backward()\n",
        "            with torch.no_grad():\n",
        "                #replace me for gradient descent updates\n",
        "        plt.plot(np.array(losses))\n",
        "        plt.yscale('log'); plt.xlabel('Epochs'); plt.ylabel('Log Loss');\n",
        "\n",
        "        print(f'Final loss = {loss}')\n",
        "        return losses\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (c) Train and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net_orig = RobustNetwork(nodes=100)\n",
        "\n",
        "# Duplicate the network for apples-to-apples comparison\n",
        "net = copy.deepcopy(net_orig) #network without regularization\n",
        "dnet = copy.deepcopy(net_orig) #network with dropout\n",
        "l2net = copy.deepcopy(net_orig) #network with l2 regularization\n",
        "\n",
        "# Set come common parameters\n",
        "lr = 0.25\n",
        "n_epochs = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# No effort to guard against overfitting\n",
        "net.drop_lyr1.dropprob = 0.\n",
        "net.drop_lyr2.dropprob = 0.\n",
        "losses = net.learn(train.inputs(), train.targets(), epochs=n_epochs, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\n",
        "#network with dropout (dnet) training here\n",
        "dlosses = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#network with l2 regularization (l2net) training here\n",
        "l2losses = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the models\n",
        "#### Let's see what the decision boundaries look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Compute validation losses here: \"validation_loss\", \"dvalidation_loss\", \"l2validation_loss\n",
        "\n",
        "# Displaying the results\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,3,1)\n",
        "#Original model\n",
        "validation.plot(labels=net(validation.inputs())); plt.title(f'Orig Validation Loss = {validation_loss:.3f}')\n",
        "plt.subplot(1,3,2)\n",
        "#Model with dropout\n",
        "validation.plot(labels=dnet(validation.inputs())); plt.title(f'Dropout Validation Loss = {dvalidation_loss:.3f}');\n",
        "plt.subplot(1,3,3)\n",
        "#Model with L2 regularization\n",
        "validation.plot(labels=l2net(validation.inputs())); plt.title(f'L2 reg Validation Loss = {l2validation_loss:.3f}');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#Share your conclusions here or in Markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "zapata",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
